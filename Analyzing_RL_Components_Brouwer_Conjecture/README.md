# Repository Structure

This repository is organized to support the exploration and reproducibility of the experiments described in  
**Analyzing RL Components for Wagnerâ€™s Framework via Brouwerâ€™s Conjecture**.

All training and evaluation procedures were performed using **Proximal Policy Optimization (PPO)** implemented via the [**Stable-Baselines3**](https://github.com/DLR-RM/stable-baselines3) library.

The main components of the project are structured into the following subdirectories:

## ğŸ“ `envs/`
Contains the **Reinforcement Learning environments** developed to test different configurations of the framework.  
Each file corresponds to a specific environment variant (`Linear`, `Local`, `Global`).

## ğŸ“ `models/`
Includes the **trained models** for each environment and reward function (`Central`, `Complete`).  
This directory contains:
- the saved trained models;
- `.csv` logs with key training metrics such as reward evolution and convergence info.

## ğŸ“ `plots/`
Stores the **best-performing graphs obtained during the testing phase**, selected according to their reward score.  
Graphs are organized by:
- environment (`Linear`, `Local`, `Global`),
- reward function (`Central`, `Complete`),
- graph size (`11â€“15` nodes).

## ğŸ“ `results/`
Contains outputs and metrics generated during the testing phase:
- `results/best_graphs/`: `.npy` files of the best graphs obtained for each configuration;
- `.csv` files with structural graph metrics (e.g., density, clustering, number of components, etc.) recorded during testing.

---

## ğŸ› ï¸ Main Scripts

### `training.py`
This script automates the **training phase** across multiple environments, reward functions, and graph sizes.    
Results are automatically saved in:
- `models/`: trained models and training logs.

The training stops automatically when convergence is reached, based on a rolling average and variance criterion.

---

### `testing.py`
This script performs **testing and evaluation** over a user-defined number of episodes per configuration.  
It loads the trained models and:
- runs a specified number of episodes for each combination of environment, reward function, and graph size,
- selects the best-performing graph based on the final reward,
- visualizes it and saves a plot in `plots/`,
- stores the graph data in `.npy` format in `results/best_graphs/`,
- logs structural metrics in `results/`.



