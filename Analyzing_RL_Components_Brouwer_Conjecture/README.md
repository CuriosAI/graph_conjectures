# Repository Structure

This repository is organized to support the exploration and reproducibility of the experiments described in  
**Analyzing RL Components for Wagner‚Äôs Framework via Brouwer‚Äôs Conjecture**.

All training and evaluation procedures were performed using **Proximal Policy Optimization (PPO)** implemented via the [**Stable-Baselines3**](https://github.com/DLR-RM/stable-baselines3) library.

The main components of the project are structured into the following subdirectories:

## üìÅ `envs/`
Contains the **Reinforcement Learning environments** developed to test different configurations of the framework.  
Each file corresponds to a specific environment variant (`Linear`, `Local`, `Global`).

## üìÅ `models/`
Includes the **trained models** for each environment and reward function (`Central`, `Complete`).  
This directory contains:
- the saved trained models;
- `.csv` logs with key training metrics such as reward evolution and convergence info.

## üìÅ `plots/`
Stores the **best-performing graphs obtained during the testing phase**, selected according to their reward score.  
Graphs are organized by:
- environment (`Linear`, `Local`, `Global`),
- reward function (`Central`, `Complete`),
- graph size (`11‚Äì15` nodes).

## üìÅ `results/`
Contains outputs and metrics generated during the testing phase:
- `results/best_graphs/`: `.npy` files of the best graphs obtained for each configuration;
- `.csv` files with structural graph metrics (e.g., density, clustering, number of components, etc.) recorded during testing.

---

## üõ†Ô∏è Main Scripts

### `training.py`
This script automates the **training phase** across multiple environments, reward functions, and graph sizes.    
Results are automatically saved in:
- `models/`: trained models and training logs.

The training stops automatically when convergence is reached, based on a rolling average and variance criterion.

---

### `testing.py`
This script performs **testing and evaluation** over a user-defined number of episodes per configuration.  
It loads the trained models and:
- runs a specified number of episodes for each combination of environment, reward function, and graph size,
- selects the best-performing graph based on the final reward,
- visualizes it and saves a plot in `plots/`,
- stores the graph data in `.npy` format in `results/best_graphs/`,
- logs structural metrics in `results/`.

---

### `GIN.py`
This script contains the architectures used in our experiments with Wagner's algorithm. 
You can find:
- the GIN and GCN structures confronted on the laplacian dataset.
- a module called policy_layers, used to predict policies after feature extraction (done with GIN or other customizable nets).
- a module called action_predictor to combined a given feature extractor to policy_layers for predicting policies in case of Linear, Local and Global environments. Must be adapted to other environments in case of usage.




